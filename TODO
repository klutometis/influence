# -*- org -*-
* TODO stability
  need to tame the stochastic variance of subsequent solutions.
* TODO combinadics
  - http://www.markmfredrickson.com/thoughts/2010-08-06-combinadics-in-r.html
  - http://stats.stackexchange.com/questions/1286/how-can-i-obtain-some-of-all-possible-combinations-in-r
  - http://compprog.wordpress.com/2007/10/17/generating-combinations-1/
  - https://stat.ethz.ch/pipermail/r-help/2006-October/115012.html
  - http://www.omegahat.org/Combinations/
* TODO spreading activation doesn't obey conservation of energy
* TODO evidence is bizarre: conditioning variables?
* TODO solutions are stochastic
  since we probabalistically recreate the correlation table
  (pairwise.complete.obs)?
* TODO bug? orthogonal sections of the graph receive energy
  even if they're not connected to evidence. some kind of feed-back
  loop?

  It's not a bug, I suppose, if spreading activation doesn't obey the
  second law of thermodynamics.

  Is that a bug? It's contrary to our physical intuition; but I'm not
  sure connectionist makes any claims about the conservation of
  energy.

  Interesting structures are orthogonal to evidence (hooked up to
  conditions).
* TODO conditional independence by m-wise regression
  should we sample the space? especially where the data is
  pathologically empty, can try $n!$ combinations.

  in order to sample the subsets, though, we'd have to use $combn$;
  could it be a data-quality issue?
* TODO iran
  #+BEGIN_SRC R :tangle iran.R :shebang #!/usr/local/bin/R --vanilla --slave -f
    library(debug)
    source('influence.R')
    data <- read.csv('iran.csv',
                     na.string=TRUE)
    
    debug(
          levels(data$condition),
          levels(data$svsplit),
          levels(as.factor(data$SVNumerical))
          )
    
    ## data$age <-
    ##   data$Approval <-
    ##   data$clossnessToIranianIdendity <-
    ##   data$clossnessToReligiousIdendity <- 
    ##   NULL
    
    data <- transform(data,
                      insideOrOutside=as.numeric(insideOrOutside),
                      svsplit=as.numeric(svsplit),
                      condition=as.numeric(condition),
                      IR=as.numeric(IR),
                      Lion=as.numeric(Lion),
                      Simple=as.numeric(Simple),
                      flag=as.numeric(flag),
                      gender=as.numeric(gender))
    
    debug(
          levels(as.factor(data$condition)),
          levels(as.factor(data$svsplit)),
          levels(as.factor(data$SVNumerical))
          )
    
    evidence <- c('condition', 'svsplit', 'SVNumerical')
    alpha <- 0.05
    
    run.influence.experiment('iran-carrot-energy-profane-values',
                             data[which(with(data,
                                             condition == 2 &
                                             svsplit == 2 &
                                             SVNumerical == 0)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-carrot-weapon-profane-values',
                             data[which(with(data,
                                             condition == 2 &
                                             svsplit == 3 &
                                             SVNumerical == 0)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-neutral-energy-profane-values',
                             data[which(with(data,
                                             condition == 3 &
                                             svsplit == 2 &
                                             SVNumerical == 0)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-neutral-weapon-profane-values',
                             data[which(with(data,
                                             condition == 3 &
                                             svsplit == 3 &
                                             SVNumerical == 0)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-stick-energy-profane-values',
                             data[which(with(data,
                                             condition == 4 &
                                             svsplit == 2 &
                                             SVNumerical == 0)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-stick-weapon-profane-values',
                             data[which(with(data,
                                             condition == 4 &
                                             svsplit == 3 &
                                             SVNumerical == 0)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-carrot-energy-sacred-values',
                             data[which(with(data,
                                             condition == 2 &
                                             svsplit == 2 &
                                             SVNumerical == 1)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-carrot-weapon-sacred-values',
                             data[which(with(data,
                                             condition == 2 &
                                             svsplit == 3 &
                                             SVNumerical == 1)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-neutral-energy-sacred-values',
                             data[which(with(data,
                                             condition == 3 &
                                             svsplit == 2 &
                                             SVNumerical == 1)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-neutral-weapon-sacred-values',
                             data[which(with(data,
                                             condition == 3 &
                                             svsplit == 3 &
                                             SVNumerical == 1)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-stick-energy-sacred-values',
                             data[which(with(data,
                                             condition == 4 &
                                             svsplit == 2 &
                                             SVNumerical == 1)),],
                             evidence,
                             alpha)
    
    run.influence.experiment('iran-stick-weapon-sacred-values',
                             data[which(with(data,
                                             condition == 4 &
                                             svsplit == 3 &
                                             SVNumerical == 1)),],
                             evidence,
                             alpha)
    
  #+END_SRC
* TODO conditions
  #+BEGIN_SRC R :tangle conditions.R :shebang #!/usr/local/bin/R --vanilla --slave -f
    library(debug)
    data <- read.csv('iran.csv', na.string=TRUE)
    debug(data,
          data$condition,
          data[which(with(data, condition == 'carrot')),],
          subset(data, condition='carrot'),
          NULL)
    
  #+END_SRC
* DONE has.adjacency
  CLOSED: [2011-06-22 Wed 18:04]
  #+BEGIN_SRC R :tangle has-adjacency.R :shebang #!/usr/local/bin/R --vanilla --slave -f
    library(debug)
    source('example-graphs.R')
    
    make.graph <- function(...)
      structure(as.environment(list(...)),
                class='graph')
    
    cliques.with.orphans <- make.graph('1'=c('2', '4', '5', '6'),
                                       '2'=c('3', '1'),
                                       '3'=c('2', '4'),
                                       '4'=c('3', '1'),
                                       '5'=c('1'),
                                       '6'=c('1', '7'),
                                       '7'=NULL,
                                       '8'=NULL)
    
    has.adjacency <- function(graph, node)
      !is.null(graph[[node]]) || node %in% Reduce(c, graph)
    
    stopifnot(has.adjacency(cliques.with.orphans, '6'))
    stopifnot(has.adjacency(cliques.with.orphans, '7'))
    stopifnot(!has.adjacency(cliques.with.orphans, '8'))
  #+END_SRC
* TODO parallelization
  - http://benreuven.com/udiwiki/index.php?title=R_with_Amazon_MapReduce
  - http://jeffreybreen.wordpress.com/2011/01/10/segue-r-to-amazon-elastic-mapreduce-hadoop/
  - http://code.google.com/p/segue/
* TODO ~explain~, ~contradict~, ~evidence~ should be macros
  none of this fragile list stuff, and need to escape; also: ~propose~
  is superfluous: propose new nodes upon explanation, etc.

  for ~explain~: the last is ~explanandum~; everything up until then:
  ~explanators~.
* TODO debug function that quotes and evaluates expressions
* DONE CIG in R
  CLOSED: [2011-05-20 Fri 06:07]
  - CLOSING NOTE [2011-05-20 Fri 06:08] \\
    Hideous hacks abound: voran!
  #+BEGIN_QUOTE
  The adjacency set of a node j in graph G, denoted by ad j(G, j), are
  all nodes i which are directly connected to j by an edge (directed
  or undirected).
  #+END_QUOTE

  [[./kalisch-estimating-high-dimensional-dags.pdf][\cite{kalisch:2007}]]:

  #+BEGIN_QUOTE
  Algorithm 1 The PC pop-algorithm
  - INPUT: Vertex Set V, Conditional Independence Information
  - OUTPUT: Estimated skeleton C, separation sets S (only needed when
    directing the skeleton afterwards)
  - Form the complete undirected graph CxC on the vertex set V.
  - l = −1; C = CxC
  - repeat
    - l = l + 1
    - repeat
      - # Does this operate anew on the modified graph, or continue
        from the last node? `(new)' would seem to imply not starting
        from node_0, lest there be some form of infinite regress.
      - # The |adj(C, i)| at time t (Card_t) is <= Card_{t - 1}; since
        l in monotonically increasing, adj(C, i) should not be
        repeated.
      - Select a (new) ordered pair of nodes i, j that are adjacent in
        C such that |adj(C, i) \ {j}| ≥ l
      - repeat
        - Choose (new) k ⊆ adj(C, i) \ {j} with |k| = l.
        - if i and j are conditionally independent given k then
          - Delete edge i, j
          - Denote this new graph by C
          - Save k in S(i, j) and S(j, i)
        - end if
      - until edge i, j is deleted or all k ⊆ ad j(C, i) \ {j}
        with |k| = l have been chosen
    - until all ordered pairs of adjacent variables i and j such
      that |adj(C, i) \ {j}| ≥ and k ⊆ adj(C, i) \ {j} with |k| = l
      have been tested for conditional independence
  - until for each ordered pair of adjacent nodes i, j: |adj(C, i) \
    {j}| < l.
  #+END_QUOTE

  [[./spirtes-algorithm-for-fast-recovery-of-sparse-causal-graphs-1991.pdf][\cite{spirtes:1991}]]:

  #+BEGIN_QUOTE
  PC Algorithm

  Let A_C{ab} denote the set of vertices adjacent to a or to b in
  graph C, except for a and b themselves. Let U_C{ab} denote the set
  of vertices in graph C on (acyclic) undirected paths between a and
  b, except for a and b themselves. (Since the algorithm is
  continually updating C, A_C{ab} and U_C{ab} are constantly changing
  as the algorithm progresses.)

  - Form the complete undirected graph C on the vertex set V.
  - n = 0
  - repeat
    - For each pair of variables (a,b) adjacent in C, if A_C{ab} ∩
      U_C{ab} has cardinality greater than or equal to n and a, b are
      independent conditional on any subsets of A_C{ab} ∩ U_C{ab} of
      cardinality n, delete a-b from C.
    - n=n+i.
  - until for each pair of adjacent vertices a, b, A_C{ab} ∩ U_C{ab}
    is of cardinality less than n.
  - Call the resulting undirected graph F.
  - For each triple of vertices (a,b,c) such that the pair (a,b) and
    the pair (b,c) are each adjacent in F but the pair (a,c) are not
    adjacent in F, orient a-b-c as a -> b <- c if and only if a and c
    are dependent on subset of A_F{ac} ∩ U_F{ac} containing b. Output
    all graphs every ccnsistent with these orientations.
  #+END_QUOTE

  ". . . a, b are independent conditional on any subsets of A_C{ab} ∩
  U_C{ab}": is it still sufficient to correlate the residuals of x ∼ S
  and y ∼ S? but S might be multiple variables, can't it?

  That's fine, though; we can subset the data frame followingly:
  data[,S].

  Is Spirtes using "adjacent" in the sense of "connected"? In which
  case, doesn't A_C{ab} ∩ U_C{ab} always reduce to A_C{ab}? I'm going
  to ignore U_C{ab}, therefore; since it's always a superset of
  A_C{ab}.

  Since we're dealing with an undirected graph, furthermore, we can
  check for adjacency merely with the originating node; and don't need
  to do a search of the inverted map.

  Otherwise: a whence and whither map?

  No, no:

  #+BEGIN_QUOTE
  21:48 < klutometis> Here's something slightly irritating: a paper
  I'm reading defines Pab as the nodes adjacent to a and b minus a and
  b; and Uab as the nodes in all undirected paths between a and b
  minus a and b.

  21:49 < klutometis> It then goes on to reference the intersection
  between Pab and Uab; isn't Uab always a superset of Pab, though?
  And isn't the intersection of Pab and Uab always Pab?

  21:49 < jcowan> No.

  21:49 < klutometis> Damn; I knew there was some subtlety.

  21:49 < jcowan> Take a node n which is adjacent to a but not on any
  path toward b.

  21:50 < jcowan> That would be in Pab but not Uab.

  21:50 < klutometis> Ah, of course; indeed, indeed.

  21:50 < pjb> Adjacent nodes are directly connected to the reference
  node.  Nodes on a path not necessarily.

  21:51 < pjb> a-c-d-e-b   Pab = {c,e}  Uab={c,d,e}

  21:52 < jcowan> And adjacent nodes aren't necessarily on a path:

  21:52 < jcowan> z-a-c-b, z is adjacent but not on a path.

  21:53 < klutometis> Right, right; the intersection of Pab and Uab is
  of course those adjacent nodes that "point" between a and b.

  21:54 < jcowan> Or: a's gateways to b plus b's gateways to a.

  21:56 < klutometis> Right; that's a good analogy.
  #+END_QUOTE

  Here's a pre-statement, by the way, in Spirtes 1990 paper:

  #+BEGIN_QUOTE
  PC Algorithm:

  Let Aab denote the set of vertices adjacent to a or to b. Let Pab
  denote the set of vertices that are parents of a or of b. Let Uab
  denot the set of vertices on undirected paths between a and b.

  - Form the complete graph C_{-1} on the vertex set V.
  - For each pair of variables a, b adjacent in C_n:
    - If Aab ∩ Uab does not have cardinality greater than n, go to the
      next pair of vertices adjacent in C_n.
    - (beta) If Aab does have cardinality greater than n, determine if
      a, b are independent conditional on any subsets of Aab ∩ Uab of
      cardinality n + 1. If so, delete a-b from C_n.
  - Let C_{n+1} be the graph that results from this procedure applied
    to each pair of variables. Continue until a value f + 1 of n is
    reached such that (beta) is not satisfied for any pair.
  - For each triple of vertices a, b, c such that the pair a, b and
    the pair b, c are each adjacent in C_f but the pair a, c are not
    adjacent in C_f, orient a - b - c as a -> b <- c if and only if a
    and c are dependent on every subset of Aac ∩ Uac containing
    b. Output all graphs consistent with these orientations.
  #+END_QUOTE

  The 1990 paper seems to imply that we should modify a copy of C_n
  becoming C_{n+1}?

  Let's do an iterative algo from nodes $n_0 . . . n_{|C| - 1}$ through
  all $l$ where, after deleting . . . no: let's copy the graph
  (according to the 1990 paper); no: 1990 implies that you
  destructively operate on C_n.

  It's this fucking quote from the 1991 paper: "Since the algorithm is
  continually updating C, A_C{ab} and U_C{ab} are constantly changing
  as the algorithm progresses."

  That's fine, though: let's gather the pairs of adjacent vertices and
  operate on them; paths will change as we operate, I guess. I wonder
  if the result of the algo is order-sensitive.

  (Neural plasticity, fluid intelligence and taking the hard way out,
  etc.)

  #+BEGIN_SRC R :tangle example-graphs.R
    make.graph <- function(...)
      structure(as.environment(list(...)),
                class='graph')
    
    as.list.graph <- as.list.environment
    
    ## From <http://ldc.usb.ve/~gabro/teaching/CI2693/lecture11.pdf>
    gabro <- make.graph('1'=c('2', '3', '4', '5'),
                        '2'=c('1', '7'),
                        '3'=c('1', '6'),
                        '4'=c('1', '6'),
                        '5'=c('1', '6'),
                        '6'=c('3', '4', '5', '7'),
                        '7'=c('2', '6'))
    
    cliques <- make.graph('1'=c('2', '4', '5', '6'),
                          '2'=c('3', '1'),
                          '3'=c('2', '4'),
                          '4'=c('3', '1'),
                          '5'=c('1'),
                          '6'=c('1'))
    
  #+END_SRC
  
  #+BEGIN_SRC R :tangle all-paths.R :shebang #!/usr/local/bin/R --vanilla --slave -f
    source('example-graphs.R')
    
    ## This is an exponential algorithm; can we do some kind of clever
    ## dynamic optimization? It seems like the paths to whither could be
    ## whitherly memoized.
    paths <- function(graph, whence, whither) {
      paths <- NULL
      ## In this case, thank God for the unmemoized nature of `visited'!
      ## Doesn't it resemble `amb'?
      search <- function(whence, visited) {
        if (whence == whither)
          paths <<- c(list(visited), paths)
        neighbors <- graph[[whence]]
        for (neighbor in setdiff(neighbors, visited))
          search(neighbor, c(visited, neighbor))
      }
      search(whence, whence)
      paths
    }
    
    stopifnot(identical(paths(gabro, '4', '2'),
                        list(c("4", "6", "7", "2"),
                             c("4", "6", "5", "1", "2"),
                             c("4", "6", "3", "1", "2"),
                             c("4", "1", "5", "6", "7", "2"),
                             c("4", "1", "3", "6", "7", "2"),
                             c("4", "1", "2"))))
    
    stopifnot(identical(paths(cliques, '1', '5'),
                        list(c('1', '5'))))
    
  #+END_SRC


  #+BEGIN_SRC R :tangle nodes-from.R :shebang #!/usr/local/bin/R --vanilla --slave -f
    source('example-graphs.R')
    
    ## We're essentially dealing with cliques, aren't we; or strongly
    ## connected components?
    nodes.from <- function(graph, whence, whither) {
      nodes <- c()
      ## In this case, thank God for the unmemoized nature of `visited'!
      ## Doesn't it resemble `amb'?
      search <- function(whence, visited) {
        if (whence == whither)
          nodes <<- union(visited, nodes)
        neighbors <- graph[[whence]]
        for (neighbor in setdiff(neighbors, visited))
          search(neighbor, c(visited, neighbor))
      }
      search(whence, whence)
      nodes
    }
    
    stopifnot(nodes.from(gabro, '1', '5') ==
              c('1', '5', '4', '6', '3', '2', '7'))
    
    stopifnot(nodes.from(cliques, '1', '5') ==
              c('1', '5'))
    
  #+END_SRC

  #+BEGIN_SRC R :tangle all-adjacent-nodes.R :shebang #!/usr/local/bin/R --vanilla --slave -f
    source('example-graphs.R')
    
    ## Check both directions?
    is.adjacent <- function(graph, whence, whither)
      whither %in% graph[[whence]] || whence %in% graph[[whither]]
    
    ## With checks for continued adjacency, since the graph may be
    ## mutating from under our feet.
    for.each.adjacent.pair <- function(graph, f) {
      for (whence in ls(graph))
        for (whither in graph[[whence]])
          if (is.adjacent(graph, whence, whither))
            f(graph, whence, whither)
    }
    
    ## Mutate the damn thing in the middle:
    for.each.adjacent.pair(cliques,
                           function(graph, whence, whither) {
                             if (exists(whither, where=graph))
                               rm(envir=graph, list=whither)
                           })
    
    stopifnot(identical(as.list.environment(cliques),
                        list('3'=c('2', '4'),
                             '1'=c('2', '4', '5', '6'))))
    
  #+END_SRC

  #+BEGIN_SRC R :tangle cig.R :shebang #!/usr/local/bin/R --vanilla --slave -f
    source('example-graphs.R')
    source('nodes-from.R')
    source('all-adjacent-nodes.R')
    
    a <- sample(6, 1000, TRUE)
    b <- sample(6, 1000, TRUE)
    c <- sample(6, 1000, TRUE)
    d <- a
    e <- a
    f <- b
    data <- cbind(a, b, c, d, e, f)
    
    nodes <- colnames(data)
    
    make.complete.undirected.graph <- function(nodes)
      do.call(make.graph,
              structure(Reduce(function(node, graph)
                               c(list(setdiff(nodes, node)), graph),
                               nodes,
                               c(),
                               right=TRUE),
                        names=nodes))
    
    graph <- make.complete.undirected.graph(colnames(data))
    
    prune.conditional.independencies <- function(graph, cardinality=0) {
      there.exist.adjacent.nodes.of.sufficient.cardinality <- FALSE
    
      for.each.adjacent.pair(graph, function(graph, whence, whither) {
        nodes <- setdiff(intersect(union(graph[[whence]], graph[[whither]]),
                                   nodes.from(graph, whence, whither)),
                         c(whence, whither))
        if (length(nodes) >= cardinality) {
          there.exist.adjacent.nodes.of.sufficient.cardinality <<- TRUE
          for (m in cardinality:length(nodes)) {
            if (m == 0) {
                if (cor.test(data[,whence], data[,whither])$p.value < 0.05) {
                  graph[[whence]] <<- setdiff(graph[[whence]], whither)
                  graph[[whither]] <<- setdiff(graph[[whither]], whence)
                  break
                }          
            } else {
              for (subset in combn(nodes, m)) {
                residuals <- resid(lm.fit(y=data[,c(whence, whither)],
                                          x=cbind(1, data[,nodes])))
                if (cor.test(residuals[,1], residuals[,2])$p.value < 0.05) {
                  graph[[whence]] <<- setdiff(graph[[whence]], whither)
                  graph[[whither]] <<- setdiff(graph[[whither]], whence)
                  break
                }
              }
            }
          }
        }
      })
    
      if (there.exist.adjacent.nodes.of.sufficient.cardinality)
        prune.conditional.independencies(graph, cardinality + 1)
    }
    
    prune.conditional.independencies(graph)
    
    as.influence <- function(x, ...)
      UseMethod('as.influence')
    
    as.influence.graph <- function(graph, data, evidence=NULL) {
      proposals <-
        Reduce(paste,
               Map(function(node)
                   sprintf('(propose! \'%s)', node),
                   ls(graph)))
      correlation <- cor(data)
      explanations <- NULL
      contradictions <- NULL
      for.each.adjacent.pair(graph, function(graph, whence, whither) {
        ## Prune the reverse link, while we're at it.
        graph[[whither]] <<- setdiff(graph[[whither]],
                                     whence)
        if (correlation[whence, whither] >= 0)
          explanations <<-
            paste(sprintf('(explain! \'(%s) \'%s)',
                          whence,
                          whither),
                  explanations)
        else
          contradictions <<-
            paste(sprintf('(contradict! \'(%s) \'%s)',
                          whence,
                          whither),
                  contradictions)
      })
      evidence <-
        Reduce(paste,
               Map(function(evidence)
                   sprintf('(evidence! \'(%s))', evidence),
                   evidence))
      paste(proposals,
            explanations,
            contradictions,
            evidence)
    }
    
    influence <- pipe('influence random')
    cat(as.influence(graph, data, 'a'),
        file=influence)
    close(influence)
    
  #+END_SRC

* DONE connectionist
  CLOSED: [2011-02-23 Wed 01:41]
# <<connectionist>>
  since $a_j(t + 1)$ depends on $a_j(t)$, we should calculate $t + 1$
  and then update the graph atomically: we can't let the $t + 1$
  contaminate $t$.
* DONE input specification
  CLOSED: [2011-02-23 Wed 01:41]
  can we just do something like this:
  
  #+BEGIN_SRC scheme
    (a 0.5)
    (b +high)
    (b -low)
    (c evidence)
    (a b +high)
    (c b -low)
    (a c 0.5)
  #+END_SRC

  or is that an appropriate way to handle evidence? do we need
  multiple evidence? let's just have a special (pre-existing) node to
  that effect:

  #+BEGIN_SRC scheme
    (a 0.5)
    (b +high)                               ; or (b high)
    (c -low)
    (a b +high)
    (evidence c -low)
    (a evidence 0.5)
  #+END_SRC

  so: we'll have nodes (activation) and links (weight, whence,
  whither); no name for now.

  actually:

  #+BEGIN_SRC scheme
    ;;; initial activations don't matter?
    (a)
    (b 0.5)
    (c 0.9 "Howdy Doody")
    
    (a b)
    (a c -0.5)
    
    (evidence a)
    (evidence c)
    
    ;;; all this (v. supra) is bad compared to the
    ;;; ({explain,contradict,...} ...) mechanisms; what's the big deal
    ;;; about key words, anyway?
    ;;; 
    ;;; would need to have the following to simulate (explain ...), etc.
    
    ((a b) c)
    ((a c) b -0.5)
    ((a b) evidence)
    
    ;;; should it look followingly instead?
    
    (use connectionist (decay 0.5) ...)
    
    (propose a)
    (propose b 0.75)
    (propose c -0.5 "Foo")
    
    ;;; though ECHO never really sets the initial activations for
    ;;; propositions; so maybe the following:
    
    (propose a)
    (propose b "Foo")
    (propose c "Bar")
    
    (explain (a c) b)
    (contradict (b) a)
    
    ;;; or simply:
    
    (contradict b a -0.75)
    
    (data a b)
    
    ;;; or maybe:
    
    (evidence a b)
    
    ;;; hmm.
  #+END_SRC

  what about joshua bloch's idea about starting with the API first?
  (see seibel's book.) hmm.

  #+BEGIN_SRC scheme :tangle test-influence-module.scm
    (module
     influence
     *
     (import scheme
             chicken)
    
     (use srfi-69
          defstruct
          matchable
          debug)
    
     (define default-activation (make-parameter 0.01))
     (define maximum-activation (make-parameter 0.99))
     (define minimum-activation (make-parameter -0.99))
    
     (define excitatory (make-parameter 0.04))
     (define inhibitory (make-parameter -0.06))
     (define default-weight (make-parameter (excitatory)))
    
     (define default-description (make-parameter ""))
    
     (define (default-description? description)
       (string=? description (default-description)))
    
     (defstruct proposition
       name
       (activation (default-activation))
       (description (default-description))
       ;; propositions are trivially accepted, rejected or something
       ;; orthogonal? a default of #t is trivially optimistic.
       (accepted? #t)
       (evidence? #f))
    
     (define evidence
       (make-proposition
        name: 'evidence
        activation: (maximum-activation)
        description: "Evidence"
        evidence?: #t))
    
     (define (evidence? proposition)
       (eq? evidence proposition))
    
     (defstruct constraint
       (weight (default-weight))
       whither)
    
     (defstruct problem
       (propositions
        ;; should we rethink this at some point?
        (let ((propositions (make-hash-table eq? hash-by-identity))) 
          (hash-table-set! propositions 'evidence evidence)
          propositions))
       (constraints (make-hash-table eq? hash-by-identity)))
    
     (define current-problem (make-parameter (make-problem)))
    
     (define link!
       (case-lambda
        ((problem whence whither)
         (link! problem whence whither (default-weight)))
        ((problem whence whither weight)
         (let* ((whither (hash-table-ref (problem-propositions problem)
                                         whither))
                (constraint (make-constraint weight: weight
                                             whither: whither)))
           (hash-table-update!/default
            (problem-constraints problem)
            whence
            (cut cons constraint <>)
            '())))))
    
    ;;; explain, contradict should call link-symmetrically! with
    ;;; e.g. certain default weights.
     (define link-symmetrically!
       (case-lambda
        ((problem whence whither)
         (link-symmetrically! problem whence whither (default-weight)))
        ((problem whence whither weight)
         (link! problem whence whither weight)
         (link! problem whither whence weight)))) 
    
     (define cohere!
       (case-lambda
        ((cohaeretors cohaerendum)
         (cohere! cohaeretors cohaerendum (default-weight)))
        ((cohaeretors cohaerendum weight)
         (cohere! (current-problem) cohaeretors cohaerendum weight))
        ((problem cohaeretors cohaerendum weight)
         (for-each
          (cut link-symmetrically! problem cohaerendum <> weight)
          cohaeretors)
         #;
         (let ((propositions (problem-propositions problem))
               (constraints (problem-constraints problem)))
           (let ((cohaerendum (hash-table-ref propositions cohaerendum)))
             (for-each
              (lambda (cohaeretor)
                (let ((cohaeretor (hash-table-ref propositions cohaeretor)))
                  (link-symmetrically! constraints cohaeretor cohaerendum weight)))
              cohaeretors))))))
    
    ;;; or maybe explain should do the symbol lookup, etc.
     (define explain!
       (case-lambda
        ((explanators explanandum)
         (explain! (current-problem) explanators explanandum))
        ((problem explanators explanandum)
         (cohere! problem explanators explanandum (excitatory)))))
    
     (define contradict!
       (case-lambda
        ((contradictors contradictum)
         (contradict! (current-problem) contradictors contradictum))
        ((problem contradictors contradictum)
         (cohere! problem contradictors contradictum (inhibitory)))))
    
     (define evidence!
       (case-lambda
        ((evidenced)
         (evidence! (current-problem) evidenced))
        ((problem evidenced)
         (explain! evidenced 'evidence))))
    
     (define propose!
       (case-lambda
        ((name)
         (propose! name (default-description)))
        ((name description)
         (propose! name description (default-activation)))
        ((name description activation)
         (propose! (current-problem) name description activation))
        ((problem name description activation)
         (hash-table-set!
          (problem-propositions problem)
          name
          (make-proposition name: name
                            activation: activation
                            description: description)))))
    
     (define (with-problem problem thunk)
       (parameterize ((current-problem problem)) (thunk))))    
    
  #+END_SRC

  do we want to have in-place mutation of the graph, btw? if we do a
  =hash-table-copy=, we're still going to have references to nodes;
  i'm beginning to see why david did the matrix thing.

  we're going to have to deep-copy the hash table in that case; if we
  merely used lists instead of e.g. node- and link-objects, that
  wouldn't be the case.

  bizarre, isn't it, that all nodes (save evidence) start off with an
  activation of 0.01? the input activations make no
  difference. interesting!

  what about abstractions like =proposition=, =explain=, =contradict=,
  etc.?

  otherwise, if we do an in-place calculation, we still have to hold
  $t + 1$ while calculating $t$ (see [[connectionist]]).

  if we're doing symmetrical links, btw, can't we have incoming and
  outgoing hash tables? although there's a space penalty, it becomes
  trivial to calculate McClelland's $S_j$: "the sum of the excitatory
  and inhibitory influences on unit $j$."

  incoming and outgoing hash-tables are orthogonal to symmetry;
  matrix-wise, however, aren't we dealing with transposing the
  adjacency matrix?

  in fact, we'd need two outgoing and two incoming links for each
  relationship in a symmetrical graph.

  #+BEGIN_SRC scheme :tangle test-hash-table.scm :shebang #!/usr/bin/env chicken-scheme
    (use srfi-69 debug test)
    
    (let ((table (make-hash-table)))
      (hash-table-update!/default
       table
       'a
       (lambda (value)
         (cons 1 value))
       '())
      (test
       "the default is passed to lambda as-is"
       '((a 1))
       (hash-table->alist table)))
  #+END_SRC

  thagard speaks of networks, units, links; but also: elements,
  constraints that underly the units and links. also: coherence
  problem, hypotheses.

  "In coherence theories of truth, the elements are propositions
  . . ." so we also have propositions.

  "Positive constraints can be a variety of relations among
  propositions, including entailment and also more complex relations
  such as explanation. The negative constraints can include
  inconsistency, but also weaker constraints such as competition."

  "Positive constraints arise from relations of explanation and
  analogy that hold between propositions, and negative constraints
  arise either because two hypotheses contradict each other or because
  they compete with each other to explain the same evidence."

  =echo-input.lisp= has: =(defun imply (lst prop) (explain lst
  prop))=, which is at least one synonym. but, in general,
  =proposition=, =explain=, =contradict= seem to suffice for the
  examples.

  let's have: problem, proposition, constraint.

  problem contexts for proposition, explain, et.nc

  #+BEGIN_SRC scheme :tangle test-with-parameter.scm :shebang #!/usr/bin/env chicken-scheme
    (use srfi-39 test)
    
    (define current-a (make-parameter 1))
    
    (define (with-a a thunk)
      (parameterize ((current-a a)) (thunk)))
    
    (with-a 2 (lambda () (test 2 (current-a))))
    
    (test 1 (current-a))
  #+END_SRC

  the verb associated with the bipartition of E into A and R is
  "satisfy": one "satisfies" a coherence problem. no, sorry; that's
  not true: one "solves" a coherence problem. (makes sense!)

  #+BEGIN_QUOTE
  To show that a given problem is a coherence problem in this sense,
  it is necessary to specify the elements and constraints, provide an
  interpretation of acceptance and rejection, and show that solutions
  to the given problem do in fact involve satisfaction of the
  specified constraints.
  #+END_QUOTE

  #+BEGIN_SRC scheme :tangle test-oj-problem.scm :shebang #!/usr/bin/env chicken-scheme
    (include "test-influence-module.scm")
    (import files influence)
    (use debug
         srfi-1
         matchable
         format
         posix
         foof-loop
         shell
         defstruct)
    
    (define maximum-iterations (make-parameter 5000))
    
    (define decay (make-parameter 0.05))
    
    (define epsilon (make-parameter 0.001))
    
    (define initial-activation (make-parameter (default-activation)))
    
    (define (display-problem/dot problem)
      (define (activation->saturation activation)
        (- 255 (inexact->exact (floor (* (/ activation (if (positive? activation)
                                                           (maximum-activation)
                                                           (minimum-activation)))
                                         255)))))
      
      (let ((document "digraph G { graph [size=\"12!\", ratio=0.618033989]; node [style=filled]; edge [dir=none]; ~a ~a }"))
        ;; what about writing right away, and not gathering?
        (let ((propositions
               (hash-table-fold
                (problem-propositions problem)
                (lambda (name proposition nodes)
                  (cons
                   (let* ((activation (proposition-activation proposition))
                          (saturation (activation->saturation activation)))
                     (format "\"~a\" [label=\"~a\", fillcolor=\"#~a~a~a\"];"
                             name
                             (let ((description
                                    (proposition-description proposition)))
                               (if (default-description? description)
                                   name
                                   #;description
                                   name
                                   ))
                             (format "~2,48X" (if (negative? activation)
                                                  255
                                                  saturation))
                             (format "~2,48X" (if (positive? activation)
                                                  255
                                                  saturation))
                             (format "~2,48X" saturation)))
                   nodes))
                '()))
              ;; respect sign and scale with dashed/solid and thickness?
              (constraints
               (map
                (match-lambda
                    ((whence whither . weight)
                     (format "\"~a\" -> \"~a\" [style=~a];"
                             whence
                             whither
                             (if (positive? weight)
                                 "solid"
                                 "dashed"))))
                ;; it's damn-expensive to delete one edge in a
                ;; bidirectional graph; we might want to consider
                ;; switching to a unidirectional graph, simulating
                ;; bidirectionality.
                (delete-duplicates
                 (hash-table-fold
                  (problem-constraints problem)
                  (lambda (name constraints edges)
                    (append
                     (map (lambda (constraint)
                            (cons* name
                                   (proposition-name
                                    (constraint-whither constraint))
                                   (constraint-weight constraint)))
                          constraints)
                     edges))
                  '())
                 (lambda (edge-a edge-b)
                   (match-let (((a-whence a-whither . a-weight) edge-a)
                               ((b-whence b-whither . b-weight) edge-b))
                     (and (eq? a-whence b-whither)
                          (eq? b-whence a-whither))))))))
          (format #t
                  document
                  (string-join propositions)
                  (string-join constraints)))))
    
    (define (clamp low high value)
      (max low (min high value)))
    
    (define cardinality
      (case-lambda
       ((integer) (cardinality integer 10))
       ((integer base)
        (loop ((for power (up-from 0))
               (until (> (expt base power) integer)))
              => power))))
    
    (defstruct processor pre-process process post-process)
    
    (define (make-animation-processor animation)
      (let ((temp-dir (create-temporary-directory))
            (temp-digits (cardinality (maximum-iterations))))
        (let ((output-template
               (format "~~~a,48d.gif" temp-digits)))
          (make-processor
           pre-process: noop
           process:
           (lambda (problem iteration)
             (let-values
                 (((in out id)
                   (process
                    "dot" `("-Tgif"
                            "-o" ,(make-pathname temp-dir
                                                 (format output-template
                                                         iteration))))))
               (close-input-port in)
               (with-output-to-port
                   out
                 (lambda () (display-problem/dot problem)))
               (close-output-port out)))
           post-process:
           (lambda (problem)
             (run (convert "$(" find ,temp-dir -name \'*.gif\' \| sort &&
                           find ,temp-dir -name \'*.gif\' \| sort -r ")"
                           -loop 0 ,animation)))))))
    
    (define (make-time-series-processor time-series)
      (let ((time-series-data (create-temporary-file)))
        (make-processor
         pre-process: noop
         process:
         (let ((time-series-data (open-output-file time-series-data)))
           (lambda (problem iteration)
             (display (string-join
                       (map
                        (compose number->string proposition-activation)
                        (hash-table-values (problem-propositions problem))))
                      time-series-data)
             (newline time-series-data)))
         post-process:
         (lambda (problem)
           (let ((propositions (problem-propositions problem))
                 (document "set term pngcairo size 1024,768 font \",8\" enhanced crop; set output \"~a\"; plot ~a
    "))
             (let ((elements
                    (loop ((for name (in-list (hash-table-keys propositions)))
                           (for proposition
                                (in-list (hash-table-values propositions)))
                           (for column (up-from 1))
                           (for elements
                                (listing
                                 (format "\"~a\" using ~a with lines title \"~a\""
                                         time-series-data
                                         column
                                         name))))
                          => elements)))
               (let-values (((in out id) (process "gnuplot")))
                 (close-input-port in)
                 (format
                  out
                  document
                  time-series
                  (string-join elements ", "))
                 (close-output-port out))))))))
    
    (define solve!
      (case-lambda
       ((algorithm) (solve! algorithm (current-problem)))
       ((algorithm problem) (algorithm problem))))
    
    (define (connectionist . processors)
      (lambda (problem)
        (let ((propositions (problem-propositions problem))
              (constraints (problem-constraints problem)))
          (hash-table-walk
           propositions
           (lambda (name proposition)
             (if (not (proposition-evidence? proposition))
                 (proposition-activation-set! proposition (initial-activation)))))
          (for-each (lambda (processor)
                      ((processor-pre-process processor) problem))
                    processors)
          (let iterate ((iteration 0)
                        (delta +Inf))
            (if (or (< (abs delta) (epsilon))
                    (> iteration (maximum-iterations)))
                (begin
                  (for-each (lambda (processor)
                              ((processor-post-process processor) problem))
                            processors)
                  problem)
                (let ((activations
                       (hash-table-fold
                        propositions
                        (lambda (name proposition activations)
                          (let ((activation
                                 (if (proposition-evidence? proposition)
                                     (proposition-activation proposition)
                                     (let ((incoming-activation
                                            (let ((constraints (hash-table-ref constraints name)))
                                              (apply +
                                                     (map *
                                                          (map constraint-weight constraints)
                                                          (map (compose proposition-activation
                                                                        constraint-whither)
                                                               constraints)))))
                                           (activation (proposition-activation proposition)))
                                       (+ (* activation (- 1 (decay)))
                                          (* incoming-activation
                                             (if (positive? incoming-activation)
                                                 (- (maximum-activation) activation)
                                                 (- activation (minimum-activation)))))))))
                            (alist-cons name
                                        (clamp (minimum-activation)
                                               (maximum-activation)
                                               activation)
                                        activations)))
                        '()))) 
                  ;; do we need this read step, or can we simply update and take
                  ;; the delta? let's update and take the delta. no; unless we
                  ;; mutate, we need to read the delta.
                  (let ((delta
                         (apply +
                                (map (match-lambda
                                         ((name . activation)
                                          (- activation
                                             (proposition-activation
                                              (hash-table-ref propositions name)))))
                                     activations)))) 
                    (for-each (match-lambda
                                  ((name . activation)
                                   (hash-table-update! propositions
                                                       name
                                                       (lambda (proposition)
                                                         (proposition-activation-set!
                                                          proposition
                                                          activation)
                                                         proposition))))
                              activations)
                    (for-each (lambda (processor)
                                ((processor-process processor) problem iteration))
                              processors)
                    (iterate (add1 iteration)
                             delta))))))))
    
    (with-problem
     (make-problem)
     (lambda ()
       (propose! 'oj-abusive)
       (propose! 'oj-beat-nicole)
       (propose! 'oj-killed-nicole)
       (propose! 'nicole-and-ron-were-killed)
       (propose! 'blood-on-sock)
       (propose! 'blood-in-ojs-car)
       (propose! 'bloody-glove)
       (propose! 'bloody-gate)
       (propose! 'fuhrman-lied)
       (propose! 'edta-on-sock)
       (propose! 'drug-dealers-killed-nicole)
       (propose! 'the-lapd-framed-oj)
    
       (explain! '(oj-beat-nicole oj-killed-nicole) 'oj-abusive)
       (explain! '(blood-on-sock
                   blood-in-ojs-car
                   bloody-glove
                   bloody-gate
                   fuhrman-lied
                   edta-on-sock)
                 'the-lapd-framed-oj)
       (explain! '(nicole-and-ron-were-killed)
                 'drug-dealers-killed-nicole)
    
       (contradict! '(drug-dealers-killed-nicole
                      the-lapd-framed-oj)
                    'oj-killed-nicole)
    
       (evidence! '(oj-beat-nicole
                    nicole-and-ron-were-killed
                    blood-on-sock
                    blood-in-ojs-car
                    bloody-glove
                    bloody-gate
                    fuhrman-lied
                    edta-on-sock)) 
    
       (display-problem/dot
        (solve! (connectionist
                 (make-animation-processor "oj.gif")
                 (make-time-series-processor "oj-series.png"))))))
    
    #;
    (with-problem
     (make-problem)
     (lambda ()
       (propose! 'e1)
       (propose! 'e2)
       (propose! 'e3)
       (propose! 'e4)
    
       (explain! '(e3 e4) 'e1)
       (explain! '(e4) 'e2)
    
       (contradict! '(e1) 'e2)
    
       (evidence! '(e1))
    
       (display-problem/dot (solve! (current-problem)))))
    
     #;
     (with-problem
      (make-problem)
      (lambda ()
        (propose! 'E1 "In combustion, heat and light are given off.")
        (propose! 'E2 "Inflammability is transmittable from one body to another.")
        (propose! 'E3 "Combustion only occurs in the presence of pure air.")
        (propose! 'E4 "Increase in weight of a burned body is exactly equal to weight of air absorbed. ")
        (propose! 'E5 "Metals undergo calcination.")
        (propose! 'E6 "In calcination, bodies increase weight.")
        (propose! 'E7 "In calcination, volume of air diminishes.") ; 628
        (propose! 'E8 "In reduction, effervescence appears.")      ; 628
    
        (propose! 'OH1 "Pure air contains oxygen principle.") ; 625
        (propose! 'OH2 "Pure air contains matter of fire and heat (MFH).") ;
        (propose! 'OH3 "In combustion, oxygen from the air combines with the burning body.")
        (propose! 'OH4 "Oxygen has weight.")
        (propose! 'OH5 "In calcination, metals add oxygen to become calxes. ")
        (propose! 'OH6 "In reduction, oxygen is given off.") ; 628
    
        (propose! 'PH1 "Combustible bodies contain phlogiston.")     ; 624
        (propose! 'PH2 "Combustible bodies contain matter of heat.") ; 652
        (propose! 'PH3 "In combustion, phlogiston is given off.")    ; 624
        (propose! 'PH4 "Phlogiston can pass from one body to another.") ; 625
        (propose! 'PH5 "Metals contain phlogiston.")               ; 624
        (propose! 'PH6 "In calcination, phlogiston is given off.") ; 624
    
                                            ; Contradictions:
    
        (contradict! '(PH3) 'OH3)
        (contradict! '(PH6) 'OH5)           ; 652
    
                                            ; Oxygen explanations:
        (explain! '(OH1 OH2 OH3) 'E1)
                                            ; E2?
        (explain! '(OH1 OH3) 'E3)
        (explain! '(OH1 OH3 OH4) 'E4)
        (explain! '(OH1 OH5) 'E5)
        (explain! '(OH1 OH4 OH5) 'E6)
        (explain! '(OH1 OH5) 'E7)
        (explain! '(OH1 OH6) 'E8)
    
                                            ; Phlogiston explanations:
        (explain! '(PH1 PH2 PH3) 'E1)
        (explain! '(PH1 PH3 PH4) 'E2)
        (explain! '(PH5 PH6) 'E5)
    
        (evidence! '(E1 E2 E3 E4 E5 E6 E7 E8))
    
        (display-problem/dot (solve! (current-problem)))))
    
  #+END_SRC

  what does it mean to solve a coherence problem, btw? bipartition of
  propositions into accepted or rejected. if we're doing this
  in-place, we can simply set a flag on the proposition to =accepted?
  => #t=.

  think along the lines of =(solve problem solver)=, where =solver= is
  e.g. connectionist instantiated with certain parameters =(decay
  0.5)=, etc.

  #+BEGIN_SRC scheme :tangle test-fold.scm :shebang #!/usr/bin/env chicken-scheme
    (use srfi-1 debug)
    
    (debug
     (fold (lambda (a b c)
             (debug a b c)
             (+ a b c))
           0
           '(1 2)
           '(3 4)))
    
  #+END_SRC

  figure out a way to [[http://orgmode.org/manual/System_002dwide-header-arguments.html#System_002dwide-header-arguments][set the default :shebang]]; this doesn't work:

  #+BEGIN_SRC elisp
    (setq org-babel-default-header-args:scheme
          '((:shebang . "#!/usr/bin/env chicken-scheme")))
  #+END_SRC

  [[http://www.mail-archive.com/emacs-orgmode@gnu.org/msg37217.html][see this]].

  #+BEGIN_SRC scheme :tangle test-equality.scm :shebang #!/usr/bin/env chicken-scheme
    (use defstruct debug srfi-69 srfi-9)
    
    (defstruct a (b 1))
    
    (let ((table (make-hash-table eq?))
          (a (make-a)))
      (hash-table-set! table a 2)
      (hash-table-ref table a)
      (a-b-set! a 2)
      ;; this throws an error! is it doing some sort of deep comparison,
      ;; or copy-on-write?
      (hash-table-ref table a))
    
  #+END_SRC

  #+BEGIN_SRC scheme :tangle test-range.scm :shebang #!/usr/bin/env chicken-scheme
    (use debug srfi-26)
    
    (define (clamp low high value)
      (max low (min high value)))
    
    (let ((min-n -1)
          (max-n 1)
          (x 9)
          (y -9)
          (z 0))
      (debug
       (map (cut clamp min-n max-n <>)
            (list x y z))))
    
  #+END_SRC

  how to visualize $t$: time series?

  #+BEGIN_SRC gnuplot :tangle oj.gp :shebang #!/usr/bin/env gnuplot
    plot "oj.dat" using 1 with lines, \
         "oj.dat" using 2 with lines, \
         "oj.dat" using 3 with lines, \
         "oj.dat" using 4 with lines, \
         "oj.dat" using 5 with lines
    
  #+END_SRC

  #+BEGIN_SRC scheme :tangle test-mkdtemp.scm :shebang #!/usr/bin/env chicken-scheme
    (use debug)
    (import foreign)
    (foreign-declare "#include <stdlib.h>")
    (debug ((foreign-lambda c-string "mkdtemp" c-string) "/tmp/XXXXXX"))
  #+END_SRC
* [[http://en.wikipedia.org/wiki/Markov_blanket][Markov blanket]]
  In machine learning, the Markov blanket for a node A in a Bayesian
  network is the set of nodes \partial A composed of A's parents, its
  children, and its children's other parents. In a Markov network, the
  Markov blanket of a node is its set of neighbouring nodes. A Markov
  blanket may also be denoted by MB(A).

  Every set of nodes in the network is conditionally independent of A
  when conditioned on the set \partial A, that is, when conditioned on
  the Markov blanket of the node A. The probability has the Markov
  property; formally, for distinct nodes A and B:

  Pr(A | \partial A, B) = Pr(A | \partial A)

  The Markov blanket of a node contains all the variables that shield
  the node from the rest of the network. This means that the Markov
  blanket of a node is the only knowledge needed to predict the
  behaviour of that node. The term was coined by Pearl in 1988.[1]

  In a Bayesian network, the values of the parents and children of a
  node evidently give information about that node; however, its
  children's parents also have to be included, because they can be used
  to explain away the node in question.
* [[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.8544&rep=rep1&type=pdf][Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm]]
  - When ignoring the directions of a DAG, we get the skeleton of a
    DAG. In general, it is different from the conditional independence
    graph (CIG), see Section 2.1. (Thus, estimation methods for
    directed graphs cannot be easily borrowed from approaches for
    undirected CIGs.)
    - For Thagard, maybe it suffices to produce a conditional
      independence graph?
  - An interesting alternative to greedy or structurally restricted
    approaches is the PC-algorithm (after its authors, Peter and
    Clark) from Spirtes et al. (2000). It starts from a complete,
    undirected graph and deletes recursively edges based on
    conditional independence decisions.
  - More importantly, we show that consistency holds even as the
    number of nodes and neighbors increases and the size of the
    smallest non-zero partial correlations decrease as a function of
    the sample size.
  - If the equivalence class is found, the Markov Blanket of any
    variable (node) can be read of easily. Given a set of nodes V and
    suppose that M is the Markov Blanket of node X, then X is
    conditionally independent of V \M given M. Thus, M contains all
    and only the relevant features for X.
  - A probability distribution P on R p is said to be faithful with
    respect to a graph G if conditional independencies of the
    distribution can be inferred from so-called d-separation in the
    graph G and vice-versa.
  - Two DAGs are equivalent if and only if they have the same skeleton
    and the same v-structures.
  - Although the main goal is to identify the CPDAG, the skeleton
    itself already contains interesting information. In particular, if
    P is faithful with respect to a DAG G, there is an edge between
    nodes i and j in the skeleton of DAG G ⇔ for all s ⊆ V \ {i, j},
    X(i) and X( j) are conditionally dependent given {X(r) ; r ∈ s},
    (1) (Spirtes et al., 2000, Th. 3.4). This implies that if P is
    faithful with respect to a DAG G, the skeleton of the DAG G is a
    subset (or equal) to the conditional independence graph (CIG)
    corresponding to P. (The reason is that an edge in a CIG requires
    only conditional dependence given the set V \ {i, j}). More
    importantly, every edge in the skeleton indicates some strong
    dependence which cannot be explained away by accounting for other
    variables. We think, that this property is of value for
    exploratory analysis.
    - This mentions the CIGs.
  - As we will see later in more detail, estimating the CPDAG consists
    of two main parts (which will naturally structure our analysis):
    (1) Estimation of the skeleton and (2) partial orientation of
    edges. All statistical inference is done in the first part, while
    the second is just application of deterministic rules on the
    results of the first part. Therefore, we will put much more
    emphasis on the analysis of the first part. If the first part is
    done correctly, the second will never fail. If, however, there
    occur errors in the first part, the second part will be more
    sensitive to it, since it depends on the inferential results of
    part (1) in greater detail. Therefore, when dealing with a
    high-dimensional setting (large p, small n), the CPDAG is harder
    to recover than the skeleton. Moreover, the interpretation of the
    CPDAG depends much more on the global correctness of the
    graph. The interpretation of the skeleton, on the other hand,
    depends only on a local region and is thus more reliable.
  - The skeleton itself oftentimes already provides interesting
    insights, and in a high-dimensional setting it might be
    interesting to use the undirected skeleton as an alternative
    target to the CPDAG when finding a useful approximation of the
    CPDAG seems hopeless.
  - A naive strategy for finding the skeleton would be to check
    conditional independencies given all subsets s ⊆ V \ {i, j} (see
    Formula 1), that is, all partial correlations in the case of
    multivariate normal distributions as first suggested by Verma and
    J.Pearl (1991). This would become computa- tionally infeasible and
    statistically ill-posed for p larger than sample size. A much
    better approach is used by the PC-algorithm which is able to
    exploit sparseness of the graph. More precisely, we apply the part
    of the PC-algorithm that identifies the undirected edges of the
    DAG.
    - all *partial correlations in the case of multivariate normal
      distributions*
    - [[http://en.wikipedia.org/wiki/Partial_correlation][Partial correlation]]
      - Formally, the partial correlation between X and Y given a set
        of n controlling variables Z = {Z1, Z2, …, Zn}, written ρXY·Z,
        is the correlation between the residuals RX and RY resulting
        from the linear regression of X with Z and of Y with Z,
        respectively. In fact, the first-order partial correlation is
        nothing else than a difference between a correlation and the
        product of the removable correlations divided by the product
        of the coefficients of alienation of the removable
        correlations. The coefficient of alienation, and its relation
        with joint variance through correlation are available in
        Guilford (1973, pp. 344–345).
      - In O(n^3) time, another approach allows all partial
        correlations to be computed between any two variables Xi and
        Xj of a set V of cardinality n, given all others, i.e., , if
        the correlation matrix (or alternatively covariance matrix) Ω
        = (ωij), where ωij = ρXiXj, is invertible. If we define P =
        Ω−1, we have . . .
      - [[http://en.wikipedia.org/wiki/Joint_probability_distribution][Joint probability distribution]]
        - In the study of probability, given two random variables X
          and Y defined on the same probability space, the joint
          distribution for X and Y defines the probability of events
          defined in terms of both X and Y.
        - If P(X + Y) = P(X)P(Y), X and Y are independent.
      - As conditional independence test
        - See also: Fisher transformation With the assumption that all
          involved variables are multivariate Gaussian, the partial
          correlation ρXY·Z is zero if and only if X is conditionally
          independent from Y given Z.[2] This property does not hold
          in the general case. To test if a sample partial correlation
          vanishes, Fisher's z-transform of the partial correlation
          can be used: The null hypothesis is , to be tested against
          the two-tail alternative . We reject H0 with significance
          level α if: where Φ(·) is the cumulative distribution
          function of a Gaussian distribution with zero mean and unit
          standard deviation, and N is the sample size. Note that this
          z-transform is approximate and that the actual distribution
          of the sample (partial) correlation coefficient is not
          straightforward. However, an exact t-test based on a
          combination of the partial regression coefficient, the
          partial correlation coefficient and the partial variances is
          available.[3] The distribution of the sample partial
          correlation was described by Fisher.[4]
  - For finite samples, we need to estimate conditional
    independencies. We limit ourselves to the Gaus- sian case, where
    all nodes correspond to random variables with a multivariate
    normal distribution. Furthermore, we assume faithful models, which
    means that the conditional independence relations correspond to
    d-separations (and so can be read off the graph) and vice versa;
    see Section 2.1. In the Gaussian case, conditional independencies
    can be inferred from partial correlations.
    - Ok: so we need a strategy for estimating conditional
      independencies for non-parametric, distrete data.
    - it works iteratively over three parameters (features); we can
      achieve multivariability by repetition.
  - The only tuning parameter of the PC-algorithm is α, which is the
    significance level for testing partial correlations. See Section 4
    for further discussion.
  - While finding the skeleton as in Algorithm 1, we recorded the
    separation sets that made edges drop out in the variable denoted
    by S. This was not necessary for finding the skeleton itself, but
    will be essential for extending the skeleton to the equivalence
    class.
    - Theoretically, we don't need this shit for Thagard.
  - Assessing the quality of fit is not quite straightforward, since
    one has to examine simultaneously both the true positive rate
    (TPR) and false positive rate (FPR) for a meaningful
    comparison. We fol- low an approach suggested by Tsamardinos et
    al. (2006) and use the Structural Hamming Distance (SHD).
    - For coming up with the conditional independence graph, the most
      difficult problem to solve is conditional independence, I
      believe.
  - The line width of the edges in the resulting skeleton and CPDAG
    can be adjusted to correspond to the reliability of the estimated
    dependencies. (The line width is proportional to the smallest
    value of n − |k| − 3 Z(i, j, k) causing an edge, see
    also 3. Therefore, thick lines are reliable).
    - Ah, so this is different from causal effect, which can be
      positive or negative!
    - Causal effect is my main interest here, isn't it?
* [[http://cran.r-project.org/web/packages/pcalg/vignettes/pcalgDoc.pdf][Causal Inference using Graphical Models with the R Package pcalg]]
  - In the next step, we use the function pc to produce an estimate of
    the underlying causal structure. Since this function is based on
    conditional independence tests, we need to define two
    things. First, we need a function that can compute conditional
    independence tests in a way that is suitable for the data at
    hand. For standard data types (Gaussian, discrete and binary) we
    provide predefined functions. See the example section in the help
    file of pc for more details. Secondly, we need a summary of the
    data (sufficient statistic) on which the conditional independence
    function can work. Each conditional independence test can be
    performed at a certain significance level alpha. This can be
    treated as a tuning parameter. In the following code, we use the
    predefined function gaussCItest() as conditional independence test
    and create the corresponding sufficient statistic, consisting of
    the correlation matrix of the data and the sample size. Then we
    use the function pc() to estimate the causal structure and plot
    the result.
    - we need an appropriate conditional-independence-test (which
      seems to require a summary-statistic: correlation matrix, sample
      size); significance level alpha: p?
    - there is also the notion of causal effect via aka perturbation
  - Based on the inferred causal structure, we can estimate the causal
    effect of an intervention. Denote the variable corresponding to
    node i in the graph by Vi . For example, suppose, we would
    increase variable V1 by external intervention by one unit. The
    recorded increase in variable V6 is the (total) causal effect of
    V1 on V6.
  - In general, nodes in the graph represent (random) variables and
    edges represent some kind of dependence.
  - If two nodes X and Y are d-separated by a set of nodes S, then the
    corresponding random variables X and Y are conditionally
    independent given the set of random variables S.
  - . . . which was proven to reconstruct the structure of the
    underlying DAG model given a conditional independence oracle up to
    some ambiguity (equivalence class) to be discussed below. In
    practice, the conditional independence oracle is replaced by a
    statistical test for conditional independence. For situations
    without hidden variables and under some further conditions it has
    been shown that the PC algorithm using statistical tests instead
    of an independence oracle is computationally feasible and
    consistent even for very high-dimensional, sparse DAGs (see
    Kalisch and Bühlmann (2007)).
    - conditional independence oracle -> statistical test for
      conditional independence
  - user-specific conditional independence test
    - The functions skeleton, pc and fci all need the argument
      indepTest, a function of the form indepTest (x, y, S, suffStat)
      to test conditional independence relationships. This function
      must return the p-value of the conditional independence test of
      x and y given S and some information on the data in the form of
      a sufficient statistic (this might be simply the data frame with
      the original data), where x, y, S indicate column positions of
      the original data matrix.
      - This gives me some idea of what to do: p-value, etc.;
        threshold; at some point: alpha?
    - A simple way to compute the partial correlation of x and y given
      S for some data is to solve the two associated linear regression
      problems x ∼ S and y ∼ S, get the residuals, and calculate the
      correlation between the residuals. Finally, a correlation test
      between the residuals yields a p-value that can be returned.
      - Where was I just reading about this: [[http://en.wikipedia.org/wiki/Partial_correlation#As_conditional_independence_test][Fisher transformation]]?
        No, that's partial correlation; although it does talk about
        residuals.
      - See also: [[http://udel.edu/~mcdonald/statcmh.html][CMH]].
    - The argument suffStat is an arbitrary object containing several
      pieces of information that are all used within the function to
      produce the p-value. In the predefined function gaussCItest()
      for example, a list containing the correlation matrix and the
      number of observations is passed. This has the advantage that
      any favorite (e.g. robust) method of computing the correlation
      matrix can be used before partial correlations are
      computed. Oftentimes, however, it will suffice to just pass the
      complete data set in suffStat.
      - Correlation test, by the way, is: Pearson's product moment
        correlation coefficient, Kendall's tau or Spearman's rho. (See
        ~cor.test~.)
      - If S is null, correlation test on the samples themselves;
        otherwise, correlation test on the residuals.
      - ~lm.fit~ has a qr method: can we simulate this in scheme? we
        have to implement linear regression.
        - residuals: the residuals, that is response minus fitted
          values.
        - fitted.values: the fitted mean values.
        - df.residual: the residual degrees of freedom.
        - y: if requested, the response used.
        - R does [[http://en.wikipedia.org/wiki/Quantile_regression][quantile regression]], apparently: "qr".
          - Quantile regression is a type of regression analysis used
            in statistics. Whereas the method of least squares results
            in estimates that approximate the conditional mean of the
            response variable given certain values of the predictor
            variables, quantile regression results in estimates
            approximating either the median or other quantiles of the
            response variable.
        - [[en.wikipedia.org/wiki/Method_of_least_squares][Least squares]]:
          - The method of least squares is a standard approach to the
            approximate solution of overdetermined systems, i.e. sets
            of equations in which there are more equations than
            unknowns. "Least squares" means that the overall solution
            minimizes the sum of the squares of the errors made in
            solving every single equation.
          - The most important application is in data fitting. The
            best fit in the least-squares sense minimizes the sum of
            squared residuals, a residual being the difference between
            an observed value and the fitted value provided by a model.
          - Least squares problems fall into two categories: linear or
            ordinary least squares and non-linear least squares,
            depending on whether or not the residuals are linear in
            all unknowns. The linear least-squares problem occurs in
            statistical regression analysis; it has a closed-form
            solution. The non-linear problem has no closed-form
            solution and is usually solved by iterative refinement; at
            each iteration the system is approximated by a linear one,
            thus the core calculation is similar in both cases.
          - The following discussion is mostly presented in terms of
            linear functions but the use of least-squares is valid and
            practical for more general families of functions. For
            example, the Fourier series approximation of degree n is
            optimal in the least-squares sense, amongst all
            approximations in terms of trigonometric polynomials of
            degree n. Also, by iteratively applying local quadratic
            approximation to the likelihood (through the Fisher
            information), the least-squares method may be used to fit
            a generalized linear model.
          - In 1822, Gauss was able to state that the least-squares
            approach to regression analysis is optimal in the sense
            that in a linear model where the errors have a mean of
            zero, are uncorrelated, and have equal variances, the best
            linear unbiased estimator of the coefficients is the
            least-squares estimator. This result is known as the
            Gauss–Markov theorem.
          - The objective consists of adjusting the parameters of a
            model function to best fit a data set. A simple data set
            consists of n points (data pairs) (xi, yi), i = 1, ..., n,
            where xi is an independent variable and yi is a dependent
            variable whose value is found by observation.
            - x, y are independent; S is observed.
          - [[http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)][Linear least squares]]
          - [[http://en.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares][Numerical methods for linear least squares]]
        - [[http://en.wikipedia.org/wiki/Quantile_regression][Quantile regression]]
          - Quantile regression does not have this structure, and
            instead leads to problems in linear programming that can
            be solved by the simplex method.
            - [[http://en.wikipedia.org/wiki/Linear_programming][Linear programming]]
              - More formally, linear programming is a technique for
                the optimization of a linear objective function,
                subject to linear equality and linear inequality
                constraints.
              - Linear programs are problems that can be expressed in
                canonical form:

                maximize C^{T}X subject to Ax <= b and x >= 0

                where x represents the vector of variables (to be
                determined), c and b are vectors of (known)
                coefficients and A is a (known) matrix of
                coefficients. The expression to be maximized or
                minimized is called the objective function (cTx in
                this case). The equations Ax ≤ b are the constraints
                which specify a convex polytope over which the
                objective function is to be optimized. (In this
                context, two vectors are comparable when every entry
                in one is less-than or equal-to the corresponding
                entry in the other. Otherwise, they are incomparable.)
                - Holy shit: should we hook into a clean-room R
                  implementation? Sucks, though, not to implement the
                  statistics; but: Jesus.
                - [[http://en.wikipedia.org/wiki/Simplex_algorithm][Simplex algorithm]]
                  - In mathematical optimization, Dantzig's simplex
                    algorithm (or simplex method) is a popular
                    algorithm for linear programming.[1] [2][3][4][5]
                    The journal Computing in Science and Engineering
                    listed it as one of the top 10 algorithms of the
                    twentieth century.[6]
                  - Let a linear program be given by a canonical
                    tableau. The simplex algorithm proceeds by
                    performing successive pivot operations which each
                    give an improved basic feasible solution; the
                    choice of pivot element at each step is largely
                    determined by the requirement that this pivot does
                    improve the solution.
                  - In general, a linear program will not be given in
                    canonical form and an equivalent canonical tableau
                    must be found before the simplex algorithm can
                    start. This can be accomplished by the
                    introduction of artificial variables. Columns of
                    the identity matrix are added as column vectors
                    for these variables. The new tableau is in
                    canonical form but it is not equivalent to the
                    original problem. So a new objective function,
                    equal to the sum of the artificial variables, is
                    introduced and the simplex algorithm is applied to
                    find the minimum; the modified linear program is
                    called the Phase I problem.[22]
          - The fact that the algorithms of linear programming appear
            more esoteric to users may explain why quantile regression
            is not as widely used as the method of least squares.[3]
  - Input: Vertex set V, conditional independence information,
    significance level α
    Output: Estimated CPDAG G, separation sets S
    Edge types: →, −
    (P1) Form the complete undirected graph on the vertex set V
    (P2) Test conditional independence given subsets of adjacency sets at
    a given significance
    level α and delete edges if conditional independent
    (P3) Orient v-structures
    (P4) Orient remaining edges.
    - This is cool: exactly what I need to simulate.
  - Therefore, even if we have an infinite amount of observations we
    can only report on a multiset of possible causal values (it is a
    multiset rather than a set because it can contain duplicate
    values). One of these values is the true causal effect. Despite
    the inherent ambiguity, this result can oftentimes still be very
    useful, when the set has certain properties (e.g. all values are
    much larger than zero).
    - If we use this notion of causal effect for Thagard: distinguish
      between values in the multiset?
    - If we use the significance of the link, how do we distinguish
      between positive and negative significance: some kind of
      heuristic on the possible causal values (e.g. all negative, or
      some summary statistic)? We don't yet have any justification for
      the summary statistic, though.
  - ida
    - shit, though: we don't have unambiguous causal effect data
      without the resulting dag
    - It is assumed that we have observational data that are
      multivariate Gaussian and faithful to the true (but unknown)
      underlying causal DAG (without hidden variables). Under these
      assumptions, ida estimates the multiset of possible total causal
      effects of x on y, where the ∂ total causal effect is defined
      via Pearl’s do-calculus as ∂z E[Y |do(X = z)] (this value does
      not depend on z, since Gaussianity implies that conditional
      expectations are linear).
      - Pearl's do-calculus.
    - We estimate a multiset of possible total causal effects instead
      of the unique total causal effect, since it is typically
      impossible to identify the latter when the true underlying
      causal DAG is unknown (even with an infinite amount of data).
    - Since there are three undirected edges in the CPDAG, there might
      be up to 23 = 8 DAGs in the corresponding equivalence
      class. However, the undirected edges V2 −V3 −V5 can be formed to
      a new v-structure. As mentioned in section 2.1, DAGs in the
      equivalence class must have exactly the same v-structures as the
      corresponding CPDAG. Thus, V2 − V3 − V5 can only be directed as
      V2 → V3 → V5 , V2 ← V3 ← V5 or V2 ← V3 → V5 , and not as V2 → V3
      ← V5 , since this would introduce a new colliding
      v-structure. There is only one remaining undirected edge (V1 −
      V6 ), which can be directed in two ways. Thus, there are six
      valid DAGs (i.e. they have no new v-structure and no directed
      cycle) and these form the equivalence class represented by the
      CPDAG. In Fig. 7, all DAGs in the equivalence class are
      shown. DAG 6 is the true DAG.
      - Shit's getting subtle.
    - For each DAG G in the equivalence class, we apply Pearl’s
      do-calculus to estimate the total causal effect of x on y. Since
      we assume Gaussianity, this can be done via a simple linear
      regression: If y is not a parent of x, we take the regression
      coefficient of x in the regression lm(y ~ x + pa(x)), where
      pa(x) denotes the parents of x in the DAG G; if y is a parent of
      x in G, we set the estimated causal effect to zero.
    - If the equivalence class contains k DAGs, this will yield k
      estimated total causal effects. Since we do not know which DAG
      is the true causal DAG, we do not know which estimated total
      causal effect of x on y is the correct one. Therefore, we return
      the entire multiset of k estimated effects.
  - idaFast
    - In some applications it is desirable to estimate the causal effect
      of one variable on a set of response variables. In these
      situations, the function idaFast() should be used. Imagine for
      example, that we have data on several variables, that we have no
      background knowledge about the causal effects among the variables
      and that we want to estimate the causal effect of each variable
      onto each other variable. To this end, we could consider for each
      variable the problem: What is the causal effect of this variable
      on all other variables. Of course, one could solve the problem by
      using ida on each pair of variables. However, there is a more
      efficient way which uses the fact that a linear regression of a
      fixed set of explanatory variables on several different response
      variables can be computed very efficiently.
* [[https://onlinecourses.science.psu.edu/stat504/node/112][P-value and independence]]
  - The p-value is P( ≥ 0.1600) = .984, indicating that the
    conditional independence model fits extremely well. As a result,
    we will not reject this model here. However, the p-value is so
    high - doesn't it make you wonder what is going on here?
* [[http://www.bnlearn.com/examples/ci.html][Conditional independence tests]]
  Available (conditional) independence tests

  The conditional independence tests used in constraint-based algorithms
  in practice are statistical tests on the data set. Available tests
  (and the respective labels) are:
  - discrete case (multinomial distribution) 
    - mutual information: an information-theoretic distance
    measure. It's proportional to the log-likelihood ratio (they
    differ by a 2n factor) and is related to the deviance of the
    tested models. Both the asymptotic chi-square test (mi) and the
    Monte Carlo permutation test (mc-mi) are implemented.
    - shrinkage estimator for the mutual information (mi-sh): an
    improved asymptotic chi-square test based on the James-Stein
    estimator for the mutual information.
    - Pearson's X^2: the classical Pearson's X^2 test for contingency
    tables. Both the asymptotic chi-square test (x2) and the Monte
    Carlo permutation test (mc-x2) are implemented.
    - Akaike Information Criterion (aict): an experimental AIC-based
    independence test, computed comparing the mutual information and
    the expected information gain.
  - continuous case (multivariate normal distribution) 
    - linear correlation: linear correlation. Both the exact Student's
    t test (cor) and the Monte Carlo permutation test (mc-cor) are
    implemented.
    - Fisher's Z: a transformation of the linear correlation with
    asymptotic normal distribution. Used by commercial software (such
    as TETRAD II) for the PC algorithm (an R implementation is present
    in the pcalg package on CRAN). Both the asymptotic normal (zf) and
    the Monte Carlo permutation test (mc-zf) are implemented.
    - mutual information: an information-theoretic distance
    measure. Again it's proportional to the log-likelihood ratio (they
    differ by a 2n factor). Both the asymptotic chi-square test (mi-g)
    and the Monte Carlo permutation test (mc-mi-g) are implemented.
    - shrinkage estimator for the mutual information (mi-g-sh): an
    improved asymptotic chi-square test based on the James-Stein
    estimator for the mutual information.
* [[https://onlinecourses.science.psu.edu/stat504/node/113][Cochran-Mantel-Haenszel Test]]
  - This is another way to test for conditional independence, by
    exploring associations in partial tables for 2 × 2 × K tables.
* [[https://onlinecourses.science.psu.edu/stat504/node/112][Conditional independence]]
* [[http://home.vrweb.de/~martin.theus/theus.pdf][Visualizing loglinear models]]
  - We consider visual methods based on mosaic plots for interpreting
    and modeling categorical data. Categorical data are most often
    modeled using loglinear models. For certain loglinear models,
    mosaic plots have unique shapes that do not depend on the actual
    data being modeled. These shapes reflect the structure of a model,
    defined by the presence and absence of particular model
    coefficients. Displaying the expected values of a loglinear model
    allows one to incorporate the residuals of the model graphically
    and to visually judge the adequacy of the loglinear fit. This
    procedure leads to stepwise interactive graphical modeling of
    loglinear models. We show that it often results in a deeper
    understanding of the structure of the data. Linking mosaic plots
    to other inter- active displays offers additional power that
    allows the investigation of more complex dependence models than
    provided by static displays.
* [[http://www.statmethods.net/stats/frequencies.html][Frequencies and crosstabs]]
  Loglinear Models 

  You can use the loglm( ) function in the MASS package to produce
  log-linear models. For example, let's assume we have a 3-way
  contingency table based on variables A, B, and C. 

  library(MASS)
  mytable <- xtabs(~A+B+C, data=mydata) 

  We can perform the following tests:

  Mutual Independence: A, B, and C are pairwise independent.
  loglm(~A+B+C, mytable) 

  Partial Independence: A is partially independent of B and C (i.e., A
  is independent of the composite variable BC).

  loglin(~A+B+C+B*C, mytable) 

  Conditional Independence: A is independent of B, given C. 

  loglm(~A+B+C+A*C+B*C, mytable)

  No Three-Way Interaction

  loglm(~A+B+C+A*B+A*C+B*C, mytable) 

  Martin Theus and Stephan Lauer have written an excellent article on
  Visualizing Loglinear Models, using mosaic plots. There is also
  great tutorial example by Kevin Quinn on analyzing loglinear models
  via glm.

  - Crosstabs and log-linear models are coming up a lot
    w.r.t. conditional independence on categorical data.
* [[http://en.wikipedia.org/wiki/Markov_property][Markov property]]
  - A stochastic process has the Markov property if the conditional
    probability distribution of future states of the process
    (conditional on both past and present values) depends only upon
    the present state; that is, given the present, the future does not
    depend on the past. A process with this property is said to be
    Markovian or a Markov process. The most famous Markov process is
    Markov chain. Brownian motion is another well-known and important
    Markov process.
* \protect\citetitle{darroch:1980}
  - this leads us to consider Markov fields on finite graphs and from
    these considerations it turns out that it is natural to define a
    class of models, graphical models whose interpretation most
    elegantly is given as a Markov property of a certain random field
    associated with the model.
* [[http://www.r-project.org/conferences/DSC-2003/Drafts/Lauritzen.pdf][gRaphical models]] (Lauritzen)
  - Undirected graphical models
    - Traces back to statistical physics (Gibbs 1902)
    - Models for spatial interaction (Besag 1974)
    - Interpreting hierarchical log-linear models by conditional
      independence, using analogy to Markov random fields (Darroch,
      Lauritzen and Speed 1980); CoCo (Badsberg 1991)
      - Darroch, J. N., Lauritzen, S. L. and Speed, T. P.: 1980,
        Markov fields and log-linear interaction models for
        contingency tables, Annals of Statistics 8, 522–539.
    - Extending hierarchical log-linear models to include continuous
      variables (Lauritzen and Wermuth 1989); MIM (Edwards 1990).
* [[http://alexandria.tue.nl/repository/books/585017.pdf][Testing conditional independence for continuous random variables]]
  - Note that the above remark regarding the limited number of tests
    of conditional independence applies only if the control variable X
    is continuous. If X is categorical, with sufficiently many
    observations per category, it is not difficult to devise a test of
    conditional independence: for each category, a test of
    independence can be done, and these tests can be combined in
    various ways. If all three variables are categorical, log-linear
    techniques can be used (cf. Lauritzen, 1996).
    - Lauritzen, S. L. (1996). Graphical models. Oxford: Clarendon
      Press.
      - Lauritzen comes up a lot.
* [[http://www.cs.waikato.ac.nz/ml/weka/][weka]]
  - [[http://www.cs.waikato.ac.nz/~remco/weka_bn/node9.html][conditional indepedence test based structure learning]]
    - A test is performed by using any of the score metrics described
      in Section 2.1.
      - Starting with a complete undirected graph, we try to find
        conditional independencies <x,y|Z> in the data. For each pair
        of nodes x, y, we consider sets Z starting with cardinality 0,
        then 1 up to a user defined maximum. Furthermore, the set Z is
        a subset of nodes that are neighbors of both x and y. If an
        independency is identified, the edge between x and y is
        removed from the skeleton.
* [[http://www.public.asu.edu/~cbaral/cse494-598-s06/classnotes/Learning-Causality.ppt][Learning causality]]
  - Once a causal model M is formed, it defines a joint probability
    distribution P(M) over the variables in the system;
  - This distribution reflects some features of the causal structure
    Each variable must be independent of its grandparents, given the
    values of its parents
  - We may allowed to inspect a select subset OV of “observed”
    variables to ask questions about P[o], the probability
    distribution over the observations;
  - We may recover the topology D of the DAG, from features of the
    probability distribution P[o].
  - Based on variable dependencies;
  - Find all pairs of variables that are dependent of each other
    (applying standard statistical method on the database);
    - what standard statistical methods? damn.
  - Eliminate (as much as possible) indirect dependencies;
  - Determine directions of dependencies
  - For each pair of variables a and b in V, search for a set Sab such
    that (a╨b | Sab) holds in P – in other words, a and b should be
    independent in P, conditioned on Sab.
* \protect\citetitle{maathius:2009}
  - Pearl: "an associational concept is any relationship that can be
    defined in termes of a joint distribution of observed variables,
    and a causal concept is any relationship that cannot be defined
    from the distribution alone. . . . Every claim invoking causal
    concepts must be trade to some premises that invoke such concepts;
    it cannot be inferred or derived from statistical associations
    alone."
  - a DAG is typically not identifiable from observational data,
    because conditional dependencies only determine the skeleton and
    the so-called v-structures of the graph.
  - a CPDAG can be estimated in various ways, including the
    PC-algorithm (Spirtes, P., Glymour, C. and Scheines,
    R. (2000). Causation, Prediction, and Search, 2nd ed. MIT Press,
    Cambridge, MA), search and score methods (Chickering,
    D. M. (2002). Learning equivalence classes of Bayesian-network
    structures. J. Mach. Learn. Res. 2 445–498, Chickering,
    D. M. (2003). Optimal structure identification with greedy
    search. J. Mach. Learn. Res. 3 507–554, Chow, C. and Liu,
    C. (1968). Approximating discrete probability distributions with
    dependence trees. IEEE Trans. Inform. Theory 14 462–467) and
    Bayesian methods (Heckerman, D., Geiger, D. and Chickering,
    D. (1995). Learning Bayesian networks: The combination of
    knowledge and statistical data. J. Mach. Learn. Res. 20 197–243,
    Spiegelhalter, D. J., Dawid, A. P., Lauritzen, S. L. and Cowell,
    R. G. (1993). Bayesian analysis in expert systems. Statist. Sci. 8
    219–283).
* \protect\citetitle{kalisch:2007}
  - R.E. Neapolitan. Learning Bayesian Networks. Pearson Prenctice
    Hall, 2004.
* \protect\citetitle{spirtes:2000}
  - we should like an algorithm that has the same input/output
    relations as the SGS procedure for faithful distributions but
    which for sparse graphs does not require the testing of higher
    order independence relations in the discrete case, and in any case
    requires testing as few d-separation relations as possible. the
    following procedure (spirtes, glymour and scheines 1991) starts by
    forming the complete undirected graph, then "thins" that graph by
    removing edges with zero order conditional indepedence relations,
    thins again with first order conditional independence relations,
    and so on. the set of variables conditioned on need only be a
    subset of the set of variables adjacent to one or the other of the
    variables conditioned.
* \protect\citetitle{spirtes:1991}
  - finding causal relations between variables is necessary . . . for
    these purposes, it is insufficient to merely fit an empirical
    covariance matrix or find the best least squares linear estimator
    of a variable.
    - Spirtes and Clark Glymour, Department of Philosophy, CMU; btw.
    - we'd essentially be discovering a causal network upon which to
      thagardize?
  - we will represent the direct causal dependence of one variable on
    another by a directed edge from a vertex representing the causal
    variable to a vertex representing the effect variable.
    - what about the skeleton sans causal inference: covariance
      matrix?
  - social scientist . . . must therefore restrict the space of
    alternatives: 1) experimental controls, 2) prior knowledge, 3)
    features of the sample data.
  - there is no evidence that such appeals constitute a reliable
    discovery procedure.
  - "substantive knowledge" rather than sample data should determine
    the causal structure of a model.
  - among the assumptions made by factor analytics programs is that
    the data functional dependencies between variables are linear, and
    that no measured variable directly causes either measured or
    latent variables . . . they are not essential to inferring causal
    structure from sample data.
  - . . . provide reliable procedures for using sample data to
    usefully narrow the class of causal structures that are, a priori,
    possible for the data.
  - pairs (g, P) for which g is a DAG and P is a probability
    distribution on the vertices of g such that 1) for every vertex v
    and set S_v of vertices that are not descendants or parents of v,
    v and S_v are independent conditional on the parents of v; and 2)
    every independence relation in P is a consequence of the
    independence relations in 1). pairs satisfying these conditions
    can be viewed as causal structures in which the causal
    dependencies generate statistical dependencies.
    - "not descendants or parents": siblings, if connected;
      orthogonal, if not.
  - when the set of measured variables for which probabilities are
    provided in the data is such that every common cause fo a measured
    variable is itself measured, we say the structure is causally
    sufficient.
    - how are the probabilities provided? "in the data".
    - ". . . every common cause of a measured variables is itself
      measured . . ."
  - recovery problem occur when determining g, or features of g, from
    the distribution P or from samples obtained from P.
    - ah, P and samples obtained (from a possibly unknown?) P are
      distinct.
  - SGS algorithm (Spirtes, et al. 1990. Causality from probability.):
    1. start with complete undirected graph
    2. for each vertex pair (a, b), remove the undirected edge between
       a anb b iff I(a, S, b) for some subset S not containing a or
       b. call this undirected graph G.
    3. for each triple (a, b, c) such that a and b are adjacent in G,
       b and c are adjacent in G, and a and c are not adjacent in G,
       direct the edges a-b and b-c into b iff for ever set S of
       vertices containing b but not a or c, ~I(a, S, C).
       - `~' signifies `not'?
    4. output all orientations of the graph consistent with 2.
       - for coming up with a skeleton, therefore, it suffices to
         eliminate edges with conditional independence? how to
         establish conditional independence: learn the conditional
         probabilities given a dataset?
  - The module of Tetrad II that implements the SGS algorithm takes a
    covariance matrix and any background causal information that the
    user has as input, and outputs a set of causal graphs compatible
    with the background knowledge that explain the conditional
    independencies true of the covariance matrix.
    - we need to get the covariance matrix; how to get from there to
      conditional independence?
    - once we establish conditional independence (the rub), the rest
      is tractable.
  - Using a joint normal distribution on the variables of zero
    indegree, three sets of simulated data were generated, each with a
    sample size of 20,000. The covariance matrix and sample size were
    given to a version of the Tetrad II program with an implementa-
    tion of the Pc algorithm.
* Fung, R., Crawford, S. (1990). Constructor: A system for the induction of probabilistic models. In Proceedings of the American Association for Artificial Intelligence (pp. 762-769). Boston.
  - Competes with Spirtes, 1991.
* Drton, et al. Smoothness of Gaussian Conditional Independence Models
  - Steffen L. Lauritzen, Graphical models, Oxford Statistical Science
    Series, vol. 17, The Claren- don Press Oxford University Press,
    New York, 1996, Oxford Science Publications. MR1419991 (98g:62001)
  - Conditional independence (CI) is one of the most important notions
    of multi- variate statistical modelling. Many popular statistical
    models can be thought of as being defined in terms of CI
    constraints. For instance, the popular graphical models are
    obtained by identifying a considered set of random variables with
    the nodes of a graph and converting separation relations in the
    graph into CI statements [9]. Despite the use of different graphs
    and separation criteria, graphical models present only a small
    subset of the models that can be defined using conditional
    independence [18]. It is thus of interest to explore to which
    extent more general collections of CI constraints may furnish
    other well-behaved statistical models.
* [[http://en.wikipedia.org/wiki/Conditional_independence][Conditional independence]]
* [[http://people.cs.ubc.ca/~murphyk/Bayes/bnintro.html][Graphical models]]
* [[http://en.wikipedia.org/wiki/Independence_(probability_theory)][Independence]]
  - If X and Y are independent, then the expectation operator E has
    the property E[XY] = E[X]E[Y], and for the variance we have
    var(X + Y) = var(X) + var(Y), so the covariance cov(X, Y) is
    zero. (The converse of these, i.e. the proposition that if two
    random variables have a covariance of 0 they must be independent,
    is not true. See uncorrelated.)
* \protect\citetitle{verma:1990}
  - relationship between dags and dependence: given a -> b -> c, the
    parameters required are P(a), P(b|a), P(c|b); "required":
    requiring estimations.
  - P(a)P(b|a) = P(ab) = P(b)P(b|a) [P(a)P(b|a) = P(ab) because of
    compounding probabilities]
  - given {a, c} -> b: P(a), P(c), P(b|ac) [P(b|ac) = P(b|a) +
    P(b|c)?]
    - joint probability: conjunctive, multiplicative; marginal
      (unconditional): integrating (summing) the joint probabilities
      over all the outcomes of X: P(A) = P(AB) + P(AB'), etc.
    - prior probability: P(A|I), probability of A given initial
      information I; conditioning of probabilities: updating them to
      take account of (possibly new) information (Bayes' theorem).
    - conditional probability: probability of C under the condition
      that A is observed; P(C|A): "probability of C given A."
    - statistically independent: P(B|A) = P(B).
    - P(C|A) = P(AC) / P(A); P(AC) = P(C|A)P(A).
    - statistically independent: P(B) = P(B|A) = P(AB)/P(A);
      i.e. P(A)P(B) = P(AB).
    - Bayes' theorem: P(B|A) = P(A|B) * P(B) / P(A); P(A)P(B|A) =
      P(A|B)P(B); P(A|B) ~= P(B|A) iff P(A) ~= P(B).
  - I(a, b, c): "a is independent of c given b"; vs. I(a, nil, c): "a
    is marginally (unconditionally) independent of c".
  - the statistical meaning of any causal model can be described
    completely and economically by its *stratified protocol*, which is
    a list of independence statements, each asserting that a variable
    is independent of its non-descendants, given its parents.
    - produce dags negatively: by removing links based on
      independence?
  - furthermore, any independence statement that logically folows from
    the statified protocol can be graphically determined in linear
    time via the *d-separation* criterion (geiger, 89, 90)
    - uh, oh: time to learn d-separation
    - geiger, verma, pearl. 1989. d-separation: from theorems to
      algorithms.
    - geiger, pearl. 1990. logical and algorithmic properties of
      independence and their application to bayesion networks.
    - [[http://www.andrew.cmu.edu/user/scheines/tutor/d-sep.html][d-separation]]
      - can we specify an algorithm that will compute, for any
        directed graph interpreted as a linear statistical model, all
        and only those independence and conditional independence
        relations that hold for all values of the parameters (causal
        strengths)?
      - Pearl realized that uncertain information could be stored much
        more efficiently by taking advantage of conditional
        independence, and they used DAGs to encode probabilities and
        the conditional independence relations among them.
      - D-separation was the algorithm they invented to compute all
        the conditional independence relations entailed by their
        graphs.
        - Pearl, J. (1988). Probabilistic Reasoning in Intelligent
          Systems. Morgan and Kaufman, San Mateo.
      - d in d-separation and d-connection stands for dependence; if
        two variables are d-separated relative to a set of variables Z
        in a directed graph, then they are independent conditional on
        Z in all probability distributions such a graph can represent
        - how to find the distribution?
      - X and Y are independent conditional on Z if knowledge about X
        gives you no extra information about Y once you have knowledge
        of Z
      - path is active if it carries information, or dependence
      - X and Y are d-separated if all the paths that connect tham are
        inactive; X and Y are connected if any of the paths between
        them are active
        - active: connected, but reachable via directionality?
      - a path is active when every vertex on the path is active
      - paths and vertices on these paths are active or inactive
        relative to a set of other vertices Z.
      - A -> C <- B: A and B have a common effect in C, but no causal
        connection between them (think back to the Pearl paper);
        whereas: A -> C -> B; A <- C <- B; A <- C -> B: these are
        "empirically indistinguishable, hence, equally predictive";
        I(A, C, B) vs. I(A, null, B); etc.
      - when the conditioning set is empty, non-colliders are active;
        when the conditioning set is empty, colliders are inactive.
      - when Z is empty, the question of whether X and Y are
        d-separated by Z in G: are there any paths between X and Y
        that have no colliders?
        - when Z is non-empty? Also, I need to come up with the
          god-damn graph. A totally connected,, bidirectional graph to
          start out with?
      - whereas conditioning on a collider activates it, so does
        conditioning on any of it's descendants.
        - have to understand the conditioning set:
      - independent causes are made dependent by conditioning on a
        common effect (conditioning on a collider)
        - conditioning: currying the variables; (partially)
          determining random variables, and thus allowing deduction on
          their dependent variables? a -> c <- b: a, c; therefore
          b. Weird, though, because given a -> c and b -> c: a, c; we
          can't say anything about b, can we? But with this
          probabilistic thing, there's P(a), P(b), P(c|ab). Since
          they're causal, though, should we interpret them as
          necessary and sufficient, and therefore an iff-relation?
        - people distinguish between sufficient and necessary
          causation, though.
      - if G is a connected graph in which X, Y, Z are disjoint; X and
        Y are d-connected by Z in G iff there exists an undirected
        path U between some vertex in X and some vertex in Y such that
        for evey collider C on U, either C or a descendent of C is in
        Z and no non-collider on U is in Z.
      - X and Y are d-separated by Z in G iff they are not d-connected
        by Z in G.
  - the question of equivalence of caulas models reduces to the
    question of equivalence of protocols: two dags are equevalent iff
    each dag's protocol holds in the other.
  - it is difficult to process visually and it does not generalize to
    embedded causal models.
  - embedded causal models are useful for modeling theories that
    cannot be modeled via simple dags . . . if there are unobserved
    variables which cause spurious correlations between the observable
    variables it may be necessary to embed the observables in a larger
    dag containing "hidden" variables in order to build an accurate
    model.
  - it might be desirable to embed the model in a larger dag to
    satisfy some high level constraints. . . suppose that every causal
    model that fits a given set of data contains the link a -> b, but
    b is known to precede a. . . one way of avoiding this conflict is
    to hypothesize the existence of an unknown common cause, i.e. a <-
    α -> b.
  - unlink simple causal models, the statistical meaning of an
    embedded causal model cannot be completely characterized by
    dependency information alone; two dependency-equivalent causal
    models need not be equivalent in the general sense.
  - dependency equivalence is a tight enough necessary condition for
    equivalence that it permits many sound conclusions to be derived
    by graphical means.
  - previous two sections are applied to the problem of recovery of a
    causal model from statistical data.
  - canonical representation called a *pattern* for describing the
    class of all models equivalent to a given dag.
  - two DAGs that represent equivalent causal models must have the
    same adjacency structure
  - adjacency, unseparability; separability and d-separation
  - A_{ab}: ancestors of a and b (less a and b); P_{ab}: parents
  - pearl is starting to use activation without defining it, I think;
    but activation is similar to reachability, isn't it?
  - the head-to-head node
    - "head-to-head": collider; "Graphically, we say that node c is
      `head-to-head' with respect to the path from a to b because it
      connects to the heads of the two arrows."  ([[http://www.bcs.rochester.edu/people/robbie/jacobslab/cheat_sheet/conditional_ind.pdf][conditional
      independence]])
  - adjacency is a property determined solely by d-separation, hence
    remains invariant among equivalent dags.
  - another important invariant: directionality of the uncoupled
    head-to-head links (a -> b <- c are uncoupled if a and c are not
    adjacent).
    - jesus: everytime he mentions a node, keep in mind that he might
      be referring to the null set.
  - lemma 2: *if a, c, b form acb while a and b are d-separated by
    some set S but not Sc then a -> c <- b. furthermore, if a -> c <-
    b, then a and b are unseparable by any set containing c.*
  - head-to-head vs. tail-to-tail and head-to-tail
  - directionality of a certain class of links can be determined from
    d-separation alone.
  - inferring causal relationships from independence statements
  - theorem 1: *two dags are equivalent iff they have the same links and same
    uncoupled head-to-head nodes*
  - the equivalence of two causal models can be determined by a simple
    graphical criterion
  - partially directed graph: rudimentary pattern of the causal model
  - rudimentary pattern define solely in terms of d-separation:
    equivalence class of causal models has a unique pattern.
  - any extension of either pattern into a full dag that does not
    create new uncoupled head-to-head nodes will be a dag in the
    equivalence class.
  - completed pattern: directed as constrained
  - emdedded causal models: direct non-causal correlation between two
    variables.
  - in a simple dag, whenever two variables are unseparable
    . . . there is no way to represet the exstence fo an unknown
    common cause.
  - no arrow head can be added to the pattern that would 1) create a
    new uncoupled head-to-head node or 2) create a strictly directed
    cycle.
  - defines a unique pattern for every embedded dag, it does so in
    terms of d-separation conditions over subsets all of U_O, which
    might require an exponential number of tests.
  - next two lemmas: polynomial time
  - lemma 3: relationship between adjacency in the pattern and
    unseparability in the causal model; practical criterion for
    determining separability in terms of a simple d-separation test
    and a graphical test.
  - under the assumption that the observed distribution is dag
    isomorphic, theorem 1 permits the recovery of the underlying
    structure uniquely, modulo the equivalence class defined by its
    pattern. one such recovery algorithm is proposed by Spirtes et al
    90 and several alternatives are discussed in the sequel.
    - spirtes, glymour and scheines. 1990. causality from probability.
    - should skip straight to the Spirtes? there's also two Spirtes to
      contend with: causality from proabability (let's start there);
      and algorithm for fast recovery of sparse causal graphs (which
      kalisch, et al used, I believe (PC vs. SGS algorithms)).
  - input: P a sampled distribution
    - i.e. took samples of some (possibly theoretical) distribution.
* [[http://www.cs.ucla.edu/~ethan/documents/dseparation.pdf][d-separation]]
  - relatively subtle; but this document is great.
  - given two sets of variables A and B, we test if they are
    independent conditioned on a set Z of variables by checking all
    paths between each variable in A and each variably in B.
  - we say thet A is independent of B given Z I(A, Z, B) if all paths
    between each variable in A and B are closed when we condition on
    (or in other words observe) Z.
  - if any path is open, we cannot claim independence but also cannot
    claim dependence.
  - we would have to examine the conditional probability tables to
    verify the indepedence claims if there is no d-separation.
  - a path is closed if there is any sequence of vertices and edges on
    it that are close according to: etc.
    - "given": "having observed".
    - still, a bizarre bidirectionality implying necessary and
      sufficient causation (especially when conditioning on
      e.g. descendents of convergent paths).
* http://en.wikipedia.org/wiki/Causality
  - conditional statement are not statements of causality; an
    important distinction is that statements of causality require the
    antecedent to precede or coincide with the consequent in time,
    whereas conditional statements do not require this order.
    - in other words, we're not guaranteed necessity and sufficiency
      with causation.
* \protect\citetitle{verma:1990}
  - the use of a DAG both to represent probabilistic relations among
    random variables on the one hand and causal relations on the other
    . . . Reichenbach's "principle of common cause": if variables
    correlated, either one causes the other or they both share a third
    common cause
    - referring to this thagard-pearl equivocation?
#+BIBLIOGRAPHY: TODO jurabib
